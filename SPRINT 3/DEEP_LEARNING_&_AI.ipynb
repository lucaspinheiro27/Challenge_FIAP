{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Desenvolva uma versão beta do projeto utilizando bibliotecas Python para transcrição de áudios.\n"
      ],
      "metadata": {
        "id": "FmC1KGi3Ougq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWa0gj7vSCNO",
        "outputId": "12c4a040-0e89-40c8-fafd-1f1b3a05b19f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-5f1e527a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-5f1e527a\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=ab56b46431f13252ff2130154144beda484407163ca003cb630966e1e28bd4c7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vvfvhurw/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n",
            "Collecting setuptools-rust\n",
            "  Downloading setuptools_rust-1.9.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: setuptools>=62.4 in /usr/local/lib/python3.10/dist-packages (from setuptools-rust) (71.0.4)\n",
            "Collecting semantic-version<3,>=2.8.2 (from setuptools-rust)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: tomli>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from setuptools-rust) (2.0.1)\n",
            "Downloading setuptools_rust-1.9.0-py3-none-any.whl (26 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: semantic-version, setuptools-rust\n",
            "Successfully installed semantic-version-2.10.0 setuptools-rust-1.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install setuptools-rust"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import whisper\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "sVciSnFESI_-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um botão OU um input no prompt/terminal para carregar pelo menos UM\n",
        "arquivo de áudio"
      ],
      "metadata": {
        "id": "P16J0PrqOeYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer upload do arquivo\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Carregar o modelo Whisper\n",
        "model = whisper.load_model(\"large\")\n",
        "\n",
        "# Pegar o nome do arquivo de áudio carregado\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "\n",
        "# Transcrever o áudio\n",
        "result = model.transcribe(audio_file)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "zfN3K2uLXHAt",
        "outputId": "c0545385-37f2-4324-bdf0-ff70cac75854"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-730ca852-5709-4d1e-8cee-50b0748bed48\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-730ca852-5709-4d1e-8cee-50b0748bed48\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2874774.wav to 2874774.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [00:35<00:00, 86.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Candinavia Natália, bom dia. Bom dia, Natália Beatriz da TOT falando. Gostaria de falar que a Sra. Silvana é do TI. Seria ela ainda responsável pelo sistema? Só um momento. Obrigada. Silvana. Bom dia, Sra. Silvana, é Beatriz da TOT falando. Tudo bem? Bom dia, Beatriz. Tudo bom e você? Também obrigada. Sra. Silvana, a Sra. seria responsável pelo sistema na empresa Scandinavia Veículos, né? Sim. Tá. Meu contato é bem rápido. Eu falo da parte da experiência do cliente. Eu gostaria de saber somente como que está aí a experiência, acompanhar um pouco a percepção que vocês estão tendo. Esse acompanhamento é feito pelo método de NPS, onde a Sra. avalia de 0 a 10 na marca TOT, alguns pontos específicos. Dois minutinhos, tudo bem? Tá. A Sra. seria responsável pelo TI, né? Sim, uma das. Tá. Eu vou só confirmar o CNPJ, porque vocês são empresas coligadas, né? Então, uma rede. O CNPJ que eu estou falando é o 67041-1116-6000-21, ok? Isso. O e-mail da Sra. continua silvania.info arroba masatarraf.com.br Isso. Então, essa filial que você falou aí, ela é de Uberlândia. Então, a gente tem depois o CNPJ final 1-17, o 4-60 e o 5-40. E o 5-40, isso. Eu estou vendo aqui. A senhora corresponde pelos três? Isso. Tá bom, então. Quatro, né? É, por esses quatro. Tá. Eu vou iniciar, então, tá, Sra. Silvana? É numa escala de 0 a 10? Qual a probabilidade da Sra. recomendar a TOTOS a um amigo ou colega hoje? 10. 10. Teria algum comentário, algum ponto que a Sra. gostaria de anexar sobre a nota? Não. Então, para a gente aprofundar um pouco mais na melhoria, agora eu vou falar alguns pontos específicos e peço que a Sra. avalie também somente de 0 a 10, tá? Caso tenha contato. Falando do suporte técnico, como a Sra. avalia a agilidade nos processos do suporte? Você fala suporte? São os chamados que a gente faz? A gente abre aí? Isso. Oito. Tá. Falando em relação ao atendimento do agente do suporte, os analistas que lidam com vocês? Oito também. Em relação ao atendimento do comercial referente ao Executivo de Vendas? Nove. Tá. Os custos e valores dos produtos contratados? Oito. Atendimento do agente do suporte? Oito. Atendimento do administrativo financeiro referente a boletos e negociações? Dez. A satisfação com a implantação do software? A Sra. participou da implantação? Sim, participei. Como foi para a Sra.? Oito. Tá. Hoje o sistema, ele está entregando benefícios que a empresa precisa? Entrega totalmente, parcialmente ou não entrega? Entrega... qual que você falou? Entrega... Totalmente, parcialmente ou não entregou? Ah, parcialmente. Tá. Como foi a experiência da Sra. com a atualização do software? Vocês já atualizaram? Dizera-nos como ela trabalhou. Qual atualização? A de versão? Pode ser a mais recente, pode ser de versão ou de releases, né? A experiência que vocês têm. Ah, tá. Foi... nossa, foi nove. Total. Tá. Tá. Para finalizar então, peço que a Sra. valha também 0 a 10 o atendimento realizado pela TOTUS Rio Preto, a unidade responsável por vocês. O atendimento é 10, em especial o analista Marcelo Pantaleão. Tá bom. Tem algum comentário final que a Sra. gostaria de deixar? Não, não. Nenhum. Tá. Esse acompanhamento, ele ocorre a cada seis meses, tá? Passando o período, retornamos o contato. Tá. Passando o período, retornamos o contato. Como vocês são empresa de rede, é capaz que vocês recebam esses contatos, mas aí a Sra. não vai poder responder para a gente mais. Teria algum outro gestor também que possa responder para a gente? Nos outros CNPJs? Tem. Tem, né? Tem, tem, tem. Tá bom então. Sra. Silvana, passando o período de seis meses a um ano, a gente retorna o contato, tá bom? Nesse CNPJ. A Sra. teria alguma empresa que gostaria de indicar? Não, não. Não? Tá bom então. Sra. Silvana, eu agradeço que a Sra. tenha um ótimo dia e bom trabalho. Tchau, tchau. Obrigada, Beatriz. Pra você também. Tchau, tchau. Obrigada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impressão do texto contido no áudio"
      ],
      "metadata": {
        "id": "fKEnOU9SOi7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition pydub\n",
        "from speech_recognition import Recognizer, AudioFile\n",
        "from pydub import AudioSegment\n",
        "\n",
        "audio = AudioSegment.from_file(\"2874774.wav\")\n",
        "audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
        "audio.export(\"2874774_converted.wav\", format=\"wav\")\n",
        "recognizer = Recognizer()\n",
        "\n",
        "with AudioFile(\"2874774_converted.wav\") as source:\n",
        "    audio_data = recognizer.record(source)\n",
        "    text = recognizer.recognize_google(audio_data, language='pt-BR')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLdTB8CSd0WL",
        "outputId": "1c04155e-d004-4c23-90e7-c3bed685b7a6",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.7.4)\n",
            "Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.4 pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) QUANTIDADE total de palavras"
      ],
      "metadata": {
        "id": "Pp8uPfgOOBqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contar_palavras(texto):\n",
        "    \"\"\"\n",
        "    Conta a quantidade total de palavras em um texto.\n",
        "\n",
        "    Args:\n",
        "        texto: O texto a ser analisado.\n",
        "\n",
        "    Returns:\n",
        "        O número total de palavras no texto.\n",
        "    \"\"\"\n",
        "    palavras = texto.split()\n",
        "    return len(palavras)\n",
        "\n",
        "# Exemplo de uso com o texto transcrito:\n",
        "text = result[\"text\"]\n",
        "quantidade_palavras = contar_palavras(text)\n",
        "print(\"O texto tem\", quantidade_palavras, \"palavras.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylcanAFLXG7n",
        "outputId": "11d4cd85-a7da-496b-922c-996140602fa4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O texto tem 666 palavras.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) QUANTIDADE de palavras por minuto"
      ],
      "metadata": {
        "id": "HQ69CvZsNRPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a duração do áudio em minutos\n",
        "duracao_audio_segundos = len(audio) / 1000\n",
        "duracao_audio_minutos = duracao_audio_segundos / 60\n",
        "\n",
        "# Calcula a quantidade de palavras por minuto\n",
        "palavras_por_minuto = quantidade_palavras / duracao_audio_minutos\n",
        "\n",
        "print(\"Quantidade de palavras por minuto:\", palavras_por_minuto)\n",
        "print(\"Duração do áudio em minutos:\", duracao_audio_minutos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZcXv_r8XG38",
        "outputId": "4e97da53-37d6-4283-e22f-797814e339d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de palavras por minuto: 133.01378070701017\n",
            "Duração do áudio em minutos: 5.007000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "81.83241973375097 * 2.5540000000000003"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d06xsrl8XGyu",
        "outputId": "015a6f9e-1567-4b77-b6b7-545636fc5459"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "209.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Palavras listadas da mais falada para a menos falada, bem como a\n",
        "impressão da QUANTIDADE de vezes em que cada palavra aparece no texto"
      ],
      "metadata": {
        "id": "7vzXGWeYNWdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "5hFIX_FQbCP6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide o texto em palavras\n",
        "palavras = text.lower().split()\n",
        "\n",
        "# Conta a frequência de cada palavra\n",
        "contagem_palavras = Counter(palavras)\n",
        "\n",
        "# Encontra a palavra mais falada\n",
        "palavra_mais_falada = contagem_palavras.most_common(1)[0]\n",
        "\n",
        "# Encontra a palavra menos falada\n",
        "palavra_menos_falada = contagem_palavras.most_common()[:-2:-1][0]\n",
        "\n",
        "# Encontra todas as palavras com a frequência da palavra mais falada\n",
        "palavras_mais_faladas = [palavra for palavra, contagem in contagem_palavras.items() if contagem == palavra_mais_falada[1]]\n",
        "\n",
        "# Encontra todas as palavras com a frequência da palavra menos falada\n",
        "palavras_menos_faladas = [palavra for palavra, contagem in contagem_palavras.items() if contagem == palavra_menos_falada[1]]\n",
        "\n",
        "print(\"Palavra(s) mais falada(s):\", palavras_mais_faladas, \" - Quantidade:\", palavra_mais_falada[1])\n",
        "print(\"Palavra(s) menos falada(s):\", palavras_menos_faladas, \" - Quantidade:\", palavra_menos_falada[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcieqOoqXGrE",
        "outputId": "0bc9807a-54d7-4b52-9f8b-76747ce821f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavra(s) mais falada(s): ['a']  - Quantidade: 45\n",
            "Palavra(s) menos falada(s): ['candinavia', 'natália,', 'dia.', 'natália', 'silvana', 'ti.', 'ainda', 'sistema?', 'momento.', 'silvana.', 'você?', 'sistema', 'scandinavia', 'veículos,', 'sim.', 'meu', 'contato', 'bem', 'rápido.', 'falo', 'parte', 'cliente.', 'saber', 'experiência,', 'acompanhar', 'percepção', 'estão', 'tendo.', 'acompanhamento', 'feito', 'método', 'nps,', 'onde', 'marca', 'tot,', 'específicos.', 'dois', 'minutinhos,', 'ti,', 'das.', 'confirmar', 'cnpj,', 'porque', 'empresas', 'coligadas,', 'rede.', '67041-1116-6000-21,', 'ok?', 'e-mail', 'continua', 'silvania.info', 'arroba', 'masatarraf.com.br', 'essa', 'filial', 'falou', 'aí,', 'uberlândia.', 'depois', '1-17,', '4-60', '5-40.', '5-40,', 'vendo', 'aqui.', 'senhora', 'corresponde', 'pelos', 'três?', 'bom,', 'quatro,', 'é,', 'quatro.', 'iniciar,', 'tá,', 'silvana?', 'numa', 'escala', '10?', 'probabilidade', 'recomendar', 'totos', 'amigo', 'colega', 'hoje?', 'comentário,', 'ponto', 'anexar', 'sobre', 'nota?', 'aprofundar', 'melhoria,', 'agora', 'específicos', 'avalie', 'caso', 'suporte', 'técnico,', 'agilidade', 'processos', 'fala', 'chamados', 'faz?', 'abre', 'aí?', 'suporte,', 'analistas', 'lidam', 'vocês?', 'oito', 'comercial', 'executivo', 'vendas?', 'custos', 'valores', 'dos', 'produtos', 'contratados?', 'administrativo', 'financeiro', 'boletos', 'negociações?', 'dez.', 'satisfação', 'implantação', 'participou', 'implantação?', 'participei.', 'sra.?', 'hoje', 'sistema,', 'entregando', 'benefícios', 'precisa?', 'entrega', 'entrega?', 'falou?', 'entregou?', 'parcialmente.', 'atualização', 'já', 'atualizaram?', 'dizera-nos', 'trabalhou.', 'atualização?', 'versão?', 'recente,', 'versão', 'releases,', 'têm.', 'foi...', 'nossa,', 'total.', 'finalizar', 'valha', 'realizado', 'pela', 'totus', 'rio', 'preto,', 'unidade', 'vocês.', 'especial', 'analista', 'marcelo', 'pantaleão.', 'bom.', 'comentário', 'deixar?', 'nenhum.', 'acompanhamento,', 'ocorre', 'cada', 'meses,', 'rede,', 'capaz', 'recebam', 'contatos,', 'mas', 'vai', 'poder', 'mais.', 'outro', 'gestor', 'possa', 'gente?', 'outros', 'cnpjs?', 'período', 'meses', 'ano,', 'retorna', 'contato,', 'bom?', 'nesse', 'cnpj.', 'alguma', 'indicar?', 'não?', 'agradeço', 'ótimo', 'dia', 'trabalho.', 'obrigada,', 'pra']  - Quantidade: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) PERCENTUAL de vezes que as “palavras mágicas” aparecem no texto:\n",
        "Obrigado(a), Por favor, Desculpa, Desculpe, Por gentileza, Bom dia, Boa tarde,\n",
        "Boa noite, Agradeço, Agradece, Gratidão, Sinto muito, Perdão, me perdoe, com\n",
        "licença."
      ],
      "metadata": {
        "id": "dKyrteStNcIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de palavras mágicas\n",
        "palavras_magicas = [\"obrigado\", \"obrigada\", \"por favor\", \"desculpa\", \"desculpe\",\n",
        "                     \"por gentileza\", \"bom dia\", \"boa tarde\", \"boa noite\",\n",
        "                     \"agradeço\", \"agradece\", \"gratidão\", \"sinto muito\",\n",
        "                     \"perdão\", \"me perdoe\", \"com licença\"]\n",
        "\n",
        "# Conta a frequência de cada palavra mágica no texto\n",
        "contagem_palavras_magicas = {palavra: text.lower().count(palavra) for palavra in palavras_magicas}\n",
        "\n",
        "# Calcula o número total de palavras mágicas\n",
        "total_palavras_magicas = sum(contagem_palavras_magicas.values())\n",
        "\n",
        "# Calcula o percentual de palavras mágicas no texto\n",
        "percentual_palavras_magicas = (total_palavras_magicas / quantidade_palavras) * 100\n",
        "\n",
        "# Imprime o percentual\n",
        "print(\"Percentual de palavras mágicas no texto: {:.2f}%\".format(percentual_palavras_magicas))\n",
        "\n",
        "# Imprime a frequência de cada palavra mágica\n",
        "for palavra, contagem in contagem_palavras_magicas.items():\n",
        "    print(\"- {}: {} vezes\".format(palavra, contagem))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRFaZHdQa3ir",
        "outputId": "ed395abb-62d2-421a-f072-37c340c4ca3e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentual de palavras mágicas no texto: 1.35%\n",
            "- obrigado: 0 vezes\n",
            "- obrigada: 4 vezes\n",
            "- por favor: 0 vezes\n",
            "- desculpa: 0 vezes\n",
            "- desculpe: 0 vezes\n",
            "- por gentileza: 0 vezes\n",
            "- bom dia: 4 vezes\n",
            "- boa tarde: 0 vezes\n",
            "- boa noite: 0 vezes\n",
            "- agradeço: 1 vezes\n",
            "- agradece: 0 vezes\n",
            "- gratidão: 0 vezes\n",
            "- sinto muito: 0 vezes\n",
            "- perdão: 0 vezes\n",
            "- me perdoe: 0 vezes\n",
            "- com licença: 0 vezes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "exleZuvnTGGr"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}